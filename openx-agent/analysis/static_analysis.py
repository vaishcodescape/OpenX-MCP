from __future__ import annotations

import hashlib
import os
import re
from collections import Counter, defaultdict
from dataclasses import dataclass
from typing import Iterable


CODE_EXTENSIONS = {
    ".py",
    ".js",
    ".ts",
    ".tsx",
    ".jsx",
    ".go",
    ".java",
    ".rb",
    ".rs",
    ".cs",
    ".cpp",
    ".c",
    ".h",
    ".hpp",
}

AI_MARKERS = [
    "generated by",
    "do not edit",
    "ai-generated",
    "copilot",
    "chatgpt",
    "automatically generated",
]

BAD_PRACTICES = [
    (re.compile(r"\beval\("), "Use of eval can be unsafe"),
    (re.compile(r"\bexec\("), "Use of exec can be unsafe"),
    (re.compile(r"\bglobal\b"), "Global state can be risky"),
    (re.compile(r"\bprint\("), "Debug prints left in code"),
]

PERF_HINTS = [
    (re.compile(r"\bfor\b.*\bfor\b"), "Nested loops may be slow"),
    (re.compile(r"\.append\(.*\).*\.append\("), "Repeated list appends; consider prealloc"),
]

BUG_HINTS = [
    (re.compile(r"TODO|FIXME"), "Unresolved TODO/FIXME"),
    (re.compile(r"except\s*:"), "Bare except may hide errors"),
]


@dataclass
class Issue:
    category: str
    message: str
    file: str
    line: int


# Directories to skip during analysis (virtual envs, build artifacts, etc.).
SKIP_DIRS = {
    ".git", ".venv", "venv", "env", ".env",
    "node_modules", "__pycache__", ".mypy_cache", ".pytest_cache",
    "target", "build", "dist", ".tox", ".eggs", "egg-info",
    ".next", ".nuxt", "vendor", "site-packages",
}


def _iter_code_files(root: str) -> Iterable[str]:
    for base, dirs, files in os.walk(root):
        # Prune skipped directories in-place so os.walk won't descend.
        dirs[:] = [d for d in dirs if d not in SKIP_DIRS and not d.endswith(".egg-info")]
        for name in files:
            ext = os.path.splitext(name)[1].lower()
            if ext in CODE_EXTENSIONS:
                yield os.path.join(base, name)


def _read_lines(path: str) -> list[str]:
    try:
        with open(path, "r", encoding="utf-8", errors="ignore") as f:
            return f.read().splitlines()
    except OSError:
        return []


def _find_markers(lines: list[str], patterns: list[tuple[re.Pattern, str]], category: str, path: str) -> list[Issue]:
    issues: list[Issue] = []
    for idx, line in enumerate(lines, start=1):
        for pat, msg in patterns:
            if pat.search(line):
                issues.append(Issue(category=category, message=msg, file=path, line=idx))
    return issues


def _ai_generated_markers(lines: list[str], path: str) -> list[Issue]:
    issues: list[Issue] = []
    for idx, line in enumerate(lines, start=1):
        lower = line.lower()
        if any(marker in lower for marker in AI_MARKERS):
            issues.append(Issue(category="ai_generated", message="Possible AI-generated code marker", file=path, line=idx))
    return issues


def _duplicate_blocks_from_cache(path_to_lines: dict[str, list[str]], min_lines: int = 6) -> list[Issue]:
    """Detect duplicate code blocks using pre-read path->lines (avoids re-reading files)."""
    blocks: dict[str, list[tuple[str, int]]] = defaultdict(list)
    for path, raw_lines in path_to_lines.items():
        lines = [ln.strip() for ln in raw_lines if ln.strip()]
        if len(lines) < min_lines:
            continue
        for i in range(0, len(lines) - min_lines + 1):
            block = "\n".join(lines[i : i + min_lines])
            digest = hashlib.sha1(block.encode("utf-8")).hexdigest()
            blocks[digest].append((path, i + 1))

    issues: list[Issue] = []
    for locations in blocks.values():
        if len(locations) > 1:
            for path, line in locations:
                issues.append(Issue(category="duplicate_code", message="Potential duplicate code block", file=path, line=line))
    return issues


def analyze_static(root: str) -> dict[str, list[dict]]:
    """Single pass: read each file once, run markers and duplicate detection on cached lines."""
    path_to_lines: dict[str, list[str]] = {}
    for path in _iter_code_files(root):
        path_to_lines[path] = _read_lines(path)

    issues: list[Issue] = []
    for path, lines in path_to_lines.items():
        issues.extend(_find_markers(lines, BAD_PRACTICES, "bad_practice", path))
        issues.extend(_find_markers(lines, PERF_HINTS, "performance", path))
        issues.extend(_find_markers(lines, BUG_HINTS, "bug", path))
        issues.extend(_ai_generated_markers(lines, path))
    issues.extend(_duplicate_blocks_from_cache(path_to_lines))

    grouped: dict[str, list[dict]] = defaultdict(list)
    for issue in issues:
        grouped[issue.category].append(
            {"message": issue.message, "file": issue.file, "line": issue.line}
        )

    return grouped


def file_stats(root: str) -> dict[str, int]:
    counts = Counter()
    for path in _iter_code_files(root):
        ext = os.path.splitext(path)[1].lower()
        counts[ext] += 1
    return dict(counts)
