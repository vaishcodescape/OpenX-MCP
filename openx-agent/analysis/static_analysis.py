"""Static code analysis: bad practices, performance hints, bugs, AI markers, duplicates.

All analysis runs in a single ``os.walk`` pass — files are read once and the
cached lines are reused for every pattern set and the duplicate-block check.
"""

from __future__ import annotations

import hashlib
import os
import re
from collections import Counter, defaultdict
from dataclasses import dataclass
from typing import Iterable

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

CODE_EXTENSIONS: frozenset[str] = frozenset({
    ".py", ".js", ".ts", ".tsx", ".jsx",
    ".go", ".java", ".rb", ".rs", ".cs",
    ".cpp", ".c", ".h", ".hpp",
})

SKIP_DIRS: frozenset[str] = frozenset({
    ".git", ".venv", "venv", "env", ".env",
    "node_modules", "__pycache__", ".mypy_cache", ".pytest_cache",
    "target", "build", "dist", ".tox", ".eggs", "egg-info",
    ".next", ".nuxt", "vendor", "site-packages",
})

_AI_MARKERS: tuple[str, ...] = (
    "generated by", "do not edit", "ai-generated",
    "copilot", "chatgpt", "automatically generated",
)

_BAD_PRACTICES: list[tuple[re.Pattern[str], str]] = [
    (re.compile(r"\beval\("),   "Use of eval can be unsafe"),
    (re.compile(r"\bexec\("),   "Use of exec can be unsafe"),
    (re.compile(r"\bglobal\b"), "Global state can be risky"),
    (re.compile(r"\bprint\("),  "Debug print left in code"),
]

_PERF_HINTS: list[tuple[re.Pattern[str], str]] = [
    (re.compile(r"\bfor\b.*\bfor\b"),               "Nested loops may be slow"),
    (re.compile(r"\.append\(.*\).*\.append\("),     "Repeated list appends; consider pre-allocation"),
]

_BUG_HINTS: list[tuple[re.Pattern[str], str]] = [
    (re.compile(r"TODO|FIXME"),   "Unresolved TODO/FIXME"),
    (re.compile(r"except\s*:"),   "Bare except may hide errors"),
]


# ---------------------------------------------------------------------------
# Data model
# ---------------------------------------------------------------------------


@dataclass(frozen=True)
class Issue:
    category: str
    message: str
    file: str
    line: int


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------


def _iter_code_files(root: str) -> Iterable[str]:
    """Walk *root*, yielding paths of recognised code files."""
    for base, dirs, files in os.walk(root):
        dirs[:] = [
            d for d in dirs
            if d not in SKIP_DIRS and not d.endswith(".egg-info")
        ]
        for name in files:
            if os.path.splitext(name)[1].lower() in CODE_EXTENSIONS:
                yield os.path.join(base, name)


def _read_lines(path: str) -> list[str]:
    try:
        with open(path, encoding="utf-8", errors="ignore") as fh:
            return fh.read().splitlines()
    except OSError:
        return []


def _scan_patterns(
    lines: list[str],
    patterns: list[tuple[re.Pattern[str], str]],
    category: str,
    path: str,
) -> list[Issue]:
    return [
        Issue(category=category, message=msg, file=path, line=idx)
        for idx, line in enumerate(lines, start=1)
        for pat, msg in patterns
        if pat.search(line)
    ]


def _scan_ai_markers(lines: list[str], path: str) -> list[Issue]:
    return [
        Issue(category="ai_generated", message="Possible AI-generated code marker", file=path, line=idx)
        for idx, line in enumerate(lines, start=1)
        if any(marker in line.lower() for marker in _AI_MARKERS)
    ]


def _find_duplicate_blocks(
    path_to_lines: dict[str, list[str]],
    min_lines: int = 6,
) -> list[Issue]:
    """Detect exact duplicate code blocks using SHA-1 of stripped line windows."""
    buckets: dict[str, list[tuple[str, int]]] = defaultdict(list)
    for path, raw_lines in path_to_lines.items():
        lines = [ln.strip() for ln in raw_lines if ln.strip()]
        for i in range(max(0, len(lines) - min_lines + 1)):
            block = "\n".join(lines[i: i + min_lines])
            digest = hashlib.sha1(block.encode()).hexdigest()
            buckets[digest].append((path, i + 1))

    return [
        Issue(category="duplicate_code", message="Potential duplicate code block", file=path, line=line)
        for locations in buckets.values()
        if len(locations) > 1
        for path, line in locations
    ]


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------


def analyze_static(root: str) -> dict[str, list[dict]]:
    """Single-pass static analysis: read each file once, reuse for all checks.

    Returns a dict mapping category → list of ``{message, file, line}`` dicts.
    """
    path_to_lines: dict[str, list[str]] = {
        path: _read_lines(path) for path in _iter_code_files(root)
    }

    issues: list[Issue] = []
    for path, lines in path_to_lines.items():
        issues.extend(_scan_patterns(lines, _BAD_PRACTICES, "bad_practice", path))
        issues.extend(_scan_patterns(lines, _PERF_HINTS,    "performance",  path))
        issues.extend(_scan_patterns(lines, _BUG_HINTS,     "bug",          path))
        issues.extend(_scan_ai_markers(lines, path))
    issues.extend(_find_duplicate_blocks(path_to_lines))

    grouped: dict[str, list[dict]] = defaultdict(list)
    for issue in issues:
        grouped[issue.category].append(
            {"message": issue.message, "file": issue.file, "line": issue.line}
        )
    return dict(grouped)


def file_stats(root: str) -> dict[str, int]:
    """Count code files by extension under *root*."""
    counts: Counter[str] = Counter(
        os.path.splitext(path)[1].lower() for path in _iter_code_files(root)
    )
    return dict(counts)
